<section class="ev-sm-container ev-view challenge-container ">
    <div class="ev-md-container ev-card-panel ev-z-depth-5 ev-challenge-view">
        <div class="row">
            <div class="col s12">
                <span class="fs-18"><a href="http://visualqa.org/" class="med-black-link"><strong>Visual Question Answering (VQA)</strong></a></span>
            </div>
        </div>
        <div class="row">
            <div class="col s12">
                <img src="dist/images/overview.jpg" width="100%">
            </div>
        </div>
        <div class="row">
            <div class="col s12">
                <p>Recent progress in computer vision and natural language processing has demonstrated that lower-level tasks are much closer to being solved. We believe that the time is ripe to pursue higher-level tasks, one of which is <a href="http://visualqa.org/" class="dark-link w-500">Visual Question Answering (VQA)</a>, where the goal is to be able to understand the semantics of scenes well enough to be able to answer open-ended, free-form natural language questions (asked by humans) about images.</p>
                <p>To promote and measure progress in this area, we have carefully created the <a href="http://visualqa.org/" class="dark-link w-500">VQA</a> dataset of questions and answers about real images and abstract scenes. We also offer two versions of the task: open-ended and multiple-choice. Open-ended requires a system to produce a natural language answer, while multiple-choice only requires a system to pick an option out of the provided answers. This dataset partially builds on top of the recent <a href="http://mscoco.org/" class="dark-link w-500">Microsoft Common Objects in COntext (MSCOCO)</a> dataset by using its images as part of the VQA dataset. In addition, it also builds a VQA dataset on top of a new collection of 50,000 <a href="https://github.com/StanislawAntol/abstract_scenes_v002" class="dark-link w-500">abstract scenes</a> (see <a href="http://arxiv.org/abs/1505.00468" class="dark-link w-500">the paper</a> for more details).</p>
                <p>To participate in one of the challenges, you can find instructions on the <a href="http://visualqa.org/" class="dark-link w-500">VQA website</a>. In particular, please see the <a href="http://visualqa.org/" class="dark-link w-500">overview</a>, <a href="http://visualqa.org/download.html">download</a>, <a href="http://visualqa.org/evaluation.html">evaluation</a>, and <a href="http://visualqa.org/challenge.html" class="dark-link w-500" class="dark-link w-500">challenge</a> pages for more details. We also provide dataset <a href="http://visualqa.org/visualize/" class="dark-link w-500">visualization</a> and <a href="http://visualqa.org/vqa_browser/vqa_dataset.html" class="dark-link w-500">browser</a> pages to give everyone a sense of the dataset contents.</p>
            </div>
        </div>
    </div>
</section>
